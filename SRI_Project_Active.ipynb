{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SRI-Project-Active.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMGmeD4Pf76e82NVEQLQ0ya",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudiaxpreda/SRI-Project/blob/main/SRI_Project_Active.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We have split the dataset previously into:\n",
        "* labeled data - 1157 articles - this are gooing to be used as a seed for the active learning model \n",
        "* unlabeled data - 3152 articles crawled from the internet - this is the part of the dataset that still needs to be labeled using an active learning model \n",
        "\n",
        "The active learning model helps us to label fresh new article just crawled from News Sites (cybernetic/ security focus websites) in order to have a larger number of samples for the model we aim to train. The original dataset was having a smaller number of articles, aproximately 1000, to which will add 3000 new articles. \n",
        "\n",
        "For testing we will use a subset of the orginal dataset, in order to have a clear statistic of how well our solution will perform. We will compare the performance of trainindgthe models using only the the original dataset, as well as the enhanced dataset.\n",
        "\n",
        "Active learning:\n",
        "*   We will use **Entropy Sampling** - is the average level of “information”, “surprise”, or “uncertainty” inherent in the variable’s possible outcomes\n",
        "*   We will use modAL package\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "J5XnY_kL0Eyi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install modAL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0cOuvI1BOgD",
        "outputId": "52cd0b47-f7cf-4c8f-877f-b202c607d161"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: modAL in /usr/local/lib/python3.7/dist-packages (0.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.0.1)\n",
            "Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.1.5)\n",
            "Requirement already satisfied: numpy>=1.13 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.7/dist-packages (from modAL) (1.4.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->modAL) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.0->modAL) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.1.0->modAL) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->modAL) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.18->modAL) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJuQaZAUJ16p",
        "outputId": "85b8f92f-afa4-4ddb-b2b7-b4602739e074"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNC6yJhWbolc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "\n",
        "from urllib.parse import urlparse\n",
        "import seaborn as sns\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from modAL.models import ActiveLearner\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "from modAL.uncertainty import entropy_sampling\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_LABELED_DATA = 'final_labeled_data.csv'\n",
        "PATH_UNALBELED_DATA = 'new_unlabeled_data.csv'\n",
        "\n",
        "ONE_HOT_ENCODING = 1\n",
        "\n",
        "def read_data_csv(path):\n",
        "  data = pd.read_csv(path)\n",
        "  data.drop(data.columns[0], axis=1, inplace=True)\n",
        "  \n",
        "  if path == PATH_LABELED_DATA:\n",
        "    data['Relevance'] = data['Relevance'].apply(lambda item : item.upper())\n",
        "    data.loc[(data['Relevance'] == 'IRELEVANT'),'Relevance']='IRRELEVANT'\n",
        "    data.drop(data[data['Relevance'] == 'SERVICE'].index, inplace=True)\n",
        "    data.reset_index(drop=True, inplace=True)\n",
        "    \n",
        "  return data\n",
        "\n",
        "def standard_cleaning(text):\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
        "    words = text.lower().split() \n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    meaningful_words = \" \".join([w for w in words if not w in stops])\n",
        "    #element = \" \".join([lemmatizer.lemmatize(w) for w in meaningful_words])\n",
        "    return meaningful_words\n",
        "\n",
        "def prepare_dataset(data, path):\n",
        "  data['Text'] = data['Title'] + data ['Description']\n",
        "  data['Text'] = data['Text'].astype(str)\n",
        "  data['Text'] = data['Text'].apply(lambda item : standard_cleaning(item))\n",
        "  if path == PATH_LABELED_DATA:\n",
        "    data['Category'] = np.where(data['Relevance'] == 'RELEVANT', 1, 0)\n",
        "  return data\n",
        "\n",
        "def split_labeled_dataset(data, dummy):\n",
        "  if dummy == ONE_HOT_ENCODING:\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "      data['Text'], data['Category'], test_size=0.30, random_state=42)\n",
        "  else:\n",
        "    # classes = []\n",
        "    # texts =[]\n",
        "    # # for _, row in data.iterrows():\n",
        "    # #       classes.append(row['Category'])\n",
        "    # #       texts.append(row['Text'])\n",
        "\n",
        "    # X_train, X_test, y_train, y_test = train_test_split(\n",
        "    #   texts, classes, test_size=0.30, random_state=42)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "      data['Text'], data['Category'], test_size=0.30, random_state=42)\n",
        "  \n",
        "  return X_train, X_test, y_train, y_test\n",
        "\n",
        "\n",
        "def get_X(data):\n",
        "  return data['Text']\n",
        "\n",
        "def get_Y(data):\n",
        "  return data['Category']\n",
        "\n",
        "def define_active_learner(cls,X_initial, y_initial ):\n",
        "  learner = ActiveLearner(\n",
        "    estimator=cls,\n",
        "    query_strategy=preset_batch,\n",
        "    X_training=X_initial, y_training=y_initial\n",
        "  )\n",
        "  return learner\n",
        "\n",
        "def learner_input(path, dummy):\n",
        "  labeled_data = read_data_csv(PATH_LABELED_DATA)\n",
        "  labeled_data = prepare_dataset(labeled_data, PATH_LABELED_DATA)\n",
        "  X_train, X_test, y_train, y_test = split_labeled_dataset(labeled_data, dummy)\n",
        "  y_train.reset_index(drop=True, inplace=True)\n",
        "  y_test.reset_index(drop=True, inplace=True)\n",
        "  X_test.reset_index(drop=True, inplace=True)\n",
        "  X_train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "  last_index = len(X_train) - 1\n",
        "\n",
        "  vectorizer = TfidfVectorizer(lowercase=False, max_features=1500)\n",
        "  X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "  X_test = vectorizer.fit_transform(X_test).toarray()\n",
        "\n",
        "  if dummy == ONE_HOT_ENCODING:\n",
        "    ohe = OneHotEncoder(handle_unknown='ignore')\n",
        "    y_train = ohe.fit_transform(y_train.values.reshape(-1,1)).toarray()\n",
        "    y_test = ohe.fit_transform(y_test.values.reshape(-1,1)).toarray()\n",
        "  else:\n",
        "      y_train = y_train.values.ravel()\n",
        "      y_test = y_test.values.ravel()\n",
        "  \n",
        "  n_initial = 150\n",
        "  initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)\n",
        "\n",
        "  X_initial = X_train[initial_idx]\n",
        "  y_initial = y_train[initial_idx]\n",
        "\n",
        "\n",
        "  y_pool = np.delete(y_train, initial_idx, axis=0)[:last_index]\n",
        "  X_pool = np.delete(X_train, initial_idx, axis=0)[:last_index]\n",
        "\n",
        "\n",
        "\n",
        "  return X_initial, X_pool, X_test, y_initial, y_pool, y_test\n",
        "\n",
        "def query_learner(learner, X_pool, y_pool):\n",
        "  n_queries = 10\n",
        "  for idx in range(n_queries):\n",
        "    query_index, query_instance = learner.query(X_pool)\n",
        "    X = X_pool[query_index]\n",
        "    y = y_pool[query_index]\n",
        "    learner.teach(X, y)\n",
        "\n",
        "\n",
        "def get_class_weights(yp):\n",
        "    from sklearn.utils import class_weight\n",
        "    classes = []\n",
        "    # texts =[]\n",
        "\n",
        "    class_weights = class_weight.compute_class_weight( class_weight = \"balanced\",\n",
        "                                        classes = np.unique(yp),\n",
        "                                        y = yp)         \n",
        "    return {i: class_weights[i] for i in range(len(np.unique(yp)))}\n"
      ],
      "metadata": {
        "id": "v3Zshj8-b081"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clsf = SVC(probability=True)\n",
        "# X_initial, X_pool, X_test, y_initial, y_pool, y_test = learner_input(PATH_LABELED_DATA, 0)\n",
        "# learner = define_active_learner(clsf, X_initial, y_initial)\n",
        "# print(learner.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "A3u9Sxur93hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  labeled_data = read_data_csv(PATH_LABELED_DATA)\n",
        "  labeled_data = prepare_dataset(labeled_data, PATH_LABELED_DATA)\n",
        "  X_train, X_test, y_train, y_test = split_labeled_dataset(labeled_data,1)\n",
        "  y_train.reset_index(drop=True, inplace=True)\n",
        "  y_test.reset_index(drop=True, inplace=True)\n",
        "  X_test.reset_index(drop=True, inplace=True)\n",
        "  X_train.reset_index(drop=True, inplace=True)\n",
        "\n",
        "  ohe = OneHotEncoder(handle_unknown='ignore')\n",
        "  y_train = ohe.fit_transform(y_train.values.reshape(-1,1)).toarray()\n",
        "  y_test = ohe.fit_transform(y_test.values.reshape(-1,1)).toarray()\n",
        "\n",
        "  \n",
        "  vectorizer = TfidfVectorizer(lowercase=False, max_features=1500)\n",
        "  X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "  X_test = vectorizer.fit_transform(X_test).toarray()"
      ],
      "metadata": {
        "id": "ih35Oy3vXZ4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from modAL.models import ActiveLearner\n",
        "\n",
        "# assembling initial training set\n",
        "n_initial = 10\n",
        "initial_idx = np.random.choice(range(len(X_train)), size=n_initial, replace=False)\n",
        "X_t, y_t = X_train[initial_idx], y_train[initial_idx]\n",
        "\n",
        "# initialize the learner\n",
        "learner = ActiveLearner(\n",
        "    estimator=RandomForestClassifier(),\n",
        "    X_training=X_t, y_training=y_t\n",
        ")\n",
        "unqueried_score = learner.score(X_test, y_test)\n",
        "\n",
        "print('Initial prediction accuracy: %f' % unqueried_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I861YQ1JWhio",
        "outputId": "2056eeb6-c734-4576-d8ed-da6923f2fb77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial prediction accuracy: 0.602941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from modAL.uncertainty import classifier_uncertainty\n",
        "\n",
        "performance_history = [unqueried_score]\n",
        "\n",
        "# learning until the accuracy reaches a given threshold|\n",
        "while learner.score(X_train, y_train) < 0.88:\n",
        "    stream_idx = np.random.choice(range(len(X_train)))\n",
        "    if classifier_uncertainty(learner, X_train[stream_idx].reshape(1, -1)).all() >= 0.4:\n",
        "        print(X_train[stream_idx].reshape(1, -1).shape)\n",
        "        learner.teach(X_train[stream_idx].reshape(1, -1), y_train[stream_idx].reshape(1, -1 ))\n",
        "        new_score = learner.score(X_train, y_train)\n",
        "        performance_history.append(new_score)\n",
        "        print('Pixel no. %d queried, new accuracy: %f' % (stream_idx, new_score))"
      ],
      "metadata": {
        "id": "HnFfOs3VXugL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unqueried_score = learner.score(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "SOcCmJacYZkg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(unqueried_score)"
      ],
      "metadata": {
        "id": "ZD2xcyILauTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "OhFpFV-oax-p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}